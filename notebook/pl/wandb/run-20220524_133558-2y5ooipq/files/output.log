[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pytorch_lightning-1.7.0.dev0-py3.9.egg/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
Help on class Metric in module torchmetrics.metric:
class Metric(torch.nn.modules.module.Module, abc.ABC)
 |  Metric(compute_on_step: Optional[bool] = None, **kwargs: Dict[str, Any]) -> None
 |
 |  Base class for all metrics present in the Metrics API.
 |
 |  Implements ``add_state()``, ``forward()``, ``reset()`` and a few other things to
 |  handle distributed synchronization and per-step metric computation.
 |
 |  Override ``update()`` and ``compute()`` functions to implement your own metric. Use
 |  ``add_state()`` to register metric state variables which keep track of state on each
 |  call of ``update()`` and are synchronized across processes when ``compute()`` is called.
 |
 |  Note:
 |      Metric state variables can either be ``torch.Tensors`` or an empty list which can we used
 |      to store `torch.Tensors``.
 |
 |  Note:
 |      Different metrics only override ``update()`` and not ``forward()``. A call to ``update()``
 |      is valid, but it won't return the metric value at the current step. A call to ``forward()``
 |      automatically calls ``update()`` and also returns the metric value at the current step.
 |
 |  Args:
 |      compute_on_step:
 |          Forward only calls ``update()`` and returns None if this is set to False.
 |
 |          .. deprecated:: v0.8
 |              Argument has no use anymore and will be removed v0.9.
 |
 |      dist_sync_on_step:
 |          Synchronize metric state across processes at each ``forward()``
 |          before returning the value at the step.
 |
 |          .. deprecated:: v0.8
 |              Argument is deprecated and will be removed in v0.9 in favour of instead
 |              passing it in as keyword argument.
 |
 |      process_group:
 |          Specify the process group on which synchronization is called. Defaults is `None`
 |          which selects the entire world
 |
 |          .. deprecated:: v0.8
 |              Argument is deprecated and will be removed in v0.9 in favour of instead
 |              passing it in as keyword argument.
 |
 |      dist_sync_fn:
 |          Callback that performs the allgather operation on the metric state. When `None`, DDP
 |          will be used to perform the allgather.
 |
 |          .. deprecated:: v0.8
 |              Argument is deprecated and will be removed in v0.9 in favour of instead
 |              passing it in as keyword argument.
 |
 |      kwargs: additional keyword arguments, see :ref:`Metric kwargs` for more info.
 |
 |          - compute_on_cpu: If metric state should be stored on CPU during computations. Only works
 |              for list states.
 |          - dist_sync_on_step: If metric state should synchronize on ``forward()``
 |          - process_group: The process group on which the synchronization is called
 |          - dist_sync_fn: function that performs the allgather option on the metric state
 |
 |  Method resolution order:
 |      Metric
 |      torch.nn.modules.module.Module
 |      abc.ABC
 |      builtins.object
 |
 |  Methods defined here:
 |
 |  __abs__(self) -> 'Metric'
 |
 |  __add__(self, other: 'Metric') -> 'Metric'
 |
 |  __and__(self, other: 'Metric') -> 'Metric'
 |
 |  __eq__(self, other: 'Metric') -> 'Metric'
 |      Return self==value.
 |
 |  __floordiv__(self, other: 'Metric') -> 'Metric'
 |
 |  __ge__(self, other: 'Metric') -> 'Metric'
 |      Return self>=value.
 |
 |  __getitem__(self, idx: int) -> 'Metric'
 |
 |  __getstate__(self) -> Dict[str, Any]
 |
 |  __gt__(self, other: 'Metric') -> 'Metric'
 |      Return self>value.
 |
 |  __hash__(self) -> int
 |      Return hash(self).
 |
 |  __init__(self, compute_on_step: Optional[bool] = None, **kwargs: Dict[str, Any]) -> None
 |      Initializes internal Module state, shared by both nn.Module and ScriptModule.
 |
 |  __inv__(self) -> 'Metric'
 |
 |  __invert__(self) -> 'Metric'
 |
 |  __le__(self, other: 'Metric') -> 'Metric'
 |      Return self<=value.
 |
 |  __lt__(self, other: 'Metric') -> 'Metric'
 |      Return self<value.
 |
 |  __matmul__(self, other: 'Metric') -> 'Metric'
 |
 |  __mod__(self, other: 'Metric') -> 'Metric'
 |
 |  __mul__(self, other: 'Metric') -> 'Metric'
 |
 |  __ne__(self, other: 'Metric') -> 'Metric'
 |      Return self!=value.
 |
 |  __neg__(self) -> 'Metric'
 |
 |  __or__(self, other: 'Metric') -> 'Metric'
 |
 |  __pos__(self) -> 'Metric'
 |
 |  __pow__(self, other: 'Metric') -> 'Metric'
 |
 |  __radd__(self, other: 'Metric') -> 'Metric'
 |
 |  __rand__(self, other: 'Metric') -> 'Metric'
 |
 |  __rfloordiv__(self, other: 'Metric') -> 'Metric'
 |
 |  __rmatmul__(self, other: 'Metric') -> 'Metric'
 |
 |  __rmod__(self, other: 'Metric') -> 'Metric'
 |
 |  __rmul__(self, other: 'Metric') -> 'Metric'
 |
 |  __ror__(self, other: 'Metric') -> 'Metric'
 |
 |  __rpow__(self, other: 'Metric') -> 'Metric'
 |
 |  __rsub__(self, other: 'Metric') -> 'Metric'
 |
 |  __rtruediv__(self, other: 'Metric') -> 'Metric'
 |
 |  __rxor__(self, other: 'Metric') -> 'Metric'
 |
 |  __setattr__(self, name: str, value: Any) -> None
 |      Implement setattr(self, name, value).
 |
 |  __setstate__(self, state: Dict[str, Any]) -> None
 |
 |  __sub__(self, other: 'Metric') -> 'Metric'
 |
 |  __truediv__(self, other: 'Metric') -> 'Metric'
 |
 |  __xor__(self, other: 'Metric') -> 'Metric'
 |
 |  add_state(self, name: str, default: Union[list, torch.Tensor], dist_reduce_fx: Union[str, Callable, NoneType] = None, persistent: bool = False) -> None
 |      Adds metric state variable. Only used by subclasses.
 |
 |      Args:
 |          name: The name of the state variable. The variable will then be accessible at ``self.name``.
 |          default: Default value of the state; can either be a ``torch.Tensor`` or an empty list. The state will be
 |              reset to this value when ``self.reset()`` is called.
 |          dist_reduce_fx (Optional): Function to reduce state across multiple processes in distributed mode.
 |              If value is ``"sum"``, ``"mean"``, ``"cat"``, ``"min"`` or ``"max"`` we will use ``torch.sum``,
 |              ``torch.mean``, ``torch.cat``, ``torch.min`` and ``torch.max``` respectively, each with argument
 |              ``dim=0``. Note that the ``"cat"`` reduction only makes sense if the state is a list, and not
 |              a tensor. The user can also pass a custom function in this parameter.
 |          persistent (Optional): whether the state will be saved as part of the modules ``state_dict``.
 |              Default is ``False``.
 |
 |      Note:
 |          Setting ``dist_reduce_fx`` to None will return the metric state synchronized across different processes.
 |          However, there won't be any reduction function applied to the synchronized metric state.
 |
 |          The metric states would be synced as follows
 |
 |          - If the metric state is ``torch.Tensor``, the synced value will be a stacked ``torch.Tensor`` across
 |            the process dimension if the metric state was a ``torch.Tensor``. The original ``torch.Tensor`` metric
 |            state retains dimension and hence the synchronized output will be of shape ``(num_process, ...)``.
 |
 |          - If the metric state is a ``list``, the synced value will be a ``list`` containing the
 |            combined elements from all processes.
 |
 |      Note:
 |          When passing a custom function to ``dist_reduce_fx``, expect the synchronized metric state to follow
 |          the format discussed in the above note.
 |
 |      Raises:
 |          ValueError:
 |              If ``default`` is not a ``tensor`` or an ``empty list``.
 |          ValueError:
 |              If ``dist_reduce_fx`` is not callable or one of ``"mean"``, ``"sum"``, ``"cat"``, ``None``.
 |
 |  clone(self) -> 'Metric'
 |      Make a copy of the metric.
 |
 |  compute(self) -> Any
 |      Override this method to compute the final metric value from state variables synchronized across the
 |      distributed backend.
 |
 |  double(self) -> 'Metric'
 |      Method override default and prevent dtype casting.
 |
 |      Please use `metric.set_dtype(dtype)` instead.
 |
 |  float(self) -> 'Metric'
 |      Method override default and prevent dtype casting.
 |
 |      Please use `metric.set_dtype(dtype)` instead.
 |
 |  forward(self, *args: Any, **kwargs: Any) -> Any
 |      Automatically calls ``update()``.
 |
 |      Returns the metric value over inputs if ``compute_on_step`` is True.
 |
 |  half(self) -> 'Metric'
 |      Method override default and prevent dtype casting.
 |
 |      Please use `metric.set_dtype(dtype)` instead.
 |
 |  persistent(self, mode: bool = False) -> None
 |      Method for post-init to change if metric states should be saved to its state_dict.
 |
 |  reset(self) -> None
 |      This method automatically resets the metric state variables to their default value.
 |
 |  set_dtype(self, dst_type: Union[str, torch.dtype]) -> 'Metric'
 |      Special version of `type` for transferring all metric states to specific dtype
 |      Arguments:
 |          dst_type (type or string): the desired type
 |
 |  state_dict(self, destination: Dict[str, Any] = None, prefix: str = '', keep_vars: bool = False) -> Optional[Dict[str, Any]]
 |      Returns a dictionary containing a whole state of the module.
 |
 |      Both parameters and persistent buffers (e.g. running averages) are
 |      included. Keys are corresponding parameter and buffer names.
 |      Parameters and buffers set to ``None`` are not included.
 |
 |      .. warning::
 |          Currently ``state_dict()`` also accepts positional arguments for
 |          ``destination``, ``prefix`` and ``keep_vars`` in order. However,
 |          this is being deprecated and keyword arguments will be enforced in
 |          future releases.
 |
 |      .. warning::
 |          Please avoid the use of argument ``destination`` as it is not
 |          designed for end-users.
 |
 |      Args:
 |          destination (dict, optional): If provided, the state of module will
 |              be updated into the dict and the same object is returned.
 |              Otherwise, an ``OrderedDict`` will be created and returned.
 |              Default: ``None``.
 |          prefix (str, optional): a prefix added to parameter and buffer
 |              names to compose the keys in state_dict. Default: ``''``.
 |          keep_vars (bool, optional): by default the :class:`~torch.Tensor` s
 |              returned in the state dict are detached from autograd. If it's
 |              set to ``True``, detaching will not be performed.
 |              Default: ``False``.
 |
 |      Returns:
 |          dict:
 |              a dictionary containing a whole state of the module
 |
 |      Example::
 |
 |          >>> module.state_dict().keys()
 |          ['bias', 'weight']
 |
 |  sync(self, dist_sync_fn: Optional[Callable] = None, process_group: Optional[Any] = None, should_sync: bool = True, distributed_available: Optional[Callable] = <function jit_distributed_available at 0x1275ba8b0>) -> None
 |      Sync function for manually controlling when metrics states should be synced across processes.
 |
 |      Args:
 |          dist_sync_fn: Function to be used to perform states synchronization
 |          process_group:
 |              Specify the process group on which synchronization is called.
 |              default: `None` (which selects the entire world)
 |          should_sync: Whether to apply to state synchronization. This will have an impact
 |              only when running in a distributed setting.
 |          distributed_available: Function to determine if we are running inside a distributed setting
 |
 |  sync_context(self, dist_sync_fn: Optional[Callable] = None, process_group: Optional[Any] = None, should_sync: bool = True, should_unsync: bool = True, distributed_available: Optional[Callable] = <function jit_distributed_available at 0x1275ba8b0>) -> Generator
 |      Context manager to synchronize the states between processes when running in a distributed setting and
 |      restore the local cache states after yielding.
 |
 |      Args:
 |          dist_sync_fn: Function to be used to perform states synchronization
 |          process_group:
 |              Specify the process group on which synchronization is called.
 |              default: `None` (which selects the entire world)
 |          should_sync: Whether to apply to state synchronization. This will have an impact
 |              only when running in a distributed setting.
 |          should_unsync: Whether to restore the cache state so that the metrics can
 |              continue to be accumulated.
 |          distributed_available: Function to determine if we are running inside a distributed setting
 |
 |  type(self, dst_type: Union[str, torch.dtype]) -> 'Metric'
 |      Method override default and prevent dtype casting.
 |
 |      Please use `metric.set_dtype(dtype)` instead.
 |
 |  unsync(self, should_unsync: bool = True) -> None
 |      Unsync function for manually controlling when metrics states should be reverted back to their local
 |      states.
 |
 |      Args:
 |          should_unsync: Whether to perform unsync
 |
 |  update(self, *_: Any, **__: Any) -> None
 |      Override this method to update the state variables of your metric class.
 |
 |  ----------------------------------------------------------------------
 |  Readonly properties defined here:
 |
 |  device
 |      Return the device of the metric.
 |
 |  ----------------------------------------------------------------------
 |  Data and other attributes defined here:
 |
 |  __abstractmethods__ = frozenset({'compute', 'update'})
 |
 |  __annotations__ = {'higher_is_better': typing.Optional[bool], 'is_diff...
 |
 |  __jit_ignored_attributes__ = ['device']
 |
 |  __jit_unused_properties__ = ['is_differentiable']
 |
 |  higher_is_better = None
 |
 |  is_differentiable = None
 |
 |  ----------------------------------------------------------------------
 |  Methods inherited from torch.nn.modules.module.Module:
 |
 |  __call__ = _call_impl(self, *input, **kwargs)
 |
 |  __delattr__(self, name)
 |      Implement delattr(self, name).
 |
 |  __dir__(self)
 |      Default dir() implementation.
 |
 |  __getattr__(self, name: str) -> Union[torch.Tensor, ForwardRef('Module')]
 |
 |  __repr__(self)
 |      Return repr(self).
 |
 |  add_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None
 |      Adds a child module to the current module.
 |
 |      The module can be accessed as an attribute using the given name.
 |
 |      Args:
 |          name (string): name of the child module. The child module can be
 |              accessed from this module using the given name
 |          module (Module): child module to be added to the module.
 |
 |  apply(self: ~T, fn: Callable[[ForwardRef('Module')], NoneType]) -> ~T
 |      Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
 |      as well as self. Typical use includes initializing the parameters of a model
 |      (see also :ref:`nn-init-doc`).
 |
 |      Args:
 |          fn (:class:`Module` -> None): function to be applied to each submodule
 |
 |      Returns:
 |          Module: self
 |
 |      Example::
 |
 |          >>> @torch.no_grad()
 |          >>> def init_weights(m):
 |          >>>     print(m)
 |          >>>     if type(m) == nn.Linear:
 |          >>>         m.weight.fill_(1.0)
 |          >>>         print(m.weight)
 |          >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
 |          >>> net.apply(init_weights)
 |          Linear(in_features=2, out_features=2, bias=True)
 |          Parameter containing:
 |          tensor([[ 1.,  1.],
 |                  [ 1.,  1.]])
 |          Linear(in_features=2, out_features=2, bias=True)
 |          Parameter containing:
 |          tensor([[ 1.,  1.],
 |                  [ 1.,  1.]])
 |          Sequential(
 |            (0): Linear(in_features=2, out_features=2, bias=True)
 |            (1): Linear(in_features=2, out_features=2, bias=True)
 |          )
 |          Sequential(
 |            (0): Linear(in_features=2, out_features=2, bias=True)
 |            (1): Linear(in_features=2, out_features=2, bias=True)
 |          )
 |
 |  bfloat16(self: ~T) -> ~T
 |      Casts all floating point parameters and buffers to ``bfloat16`` datatype.
 |
 |      .. note::
 |          This method modifies the module in-place.
 |
 |      Returns:
 |          Module: self
 |
 |  buffers(self, recurse: bool = True) -> Iterator[torch.Tensor]
 |      Returns an iterator over module buffers.
 |
 |      Args:
 |          recurse (bool): if True, then yields buffers of this module
 |              and all submodules. Otherwise, yields only buffers that
 |              are direct members of this module.
 |
 |      Yields:
 |          torch.Tensor: module buffer
 |
 |      Example::
 |
 |          >>> for buf in model.buffers():
 |          >>>     print(type(buf), buf.size())
 |          <class 'torch.Tensor'> (20L,)
 |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)
 |
 |  children(self) -> Iterator[ForwardRef('Module')]
 |      Returns an iterator over immediate children modules.
 |
 |      Yields:
 |          Module: a child module
 |
 |  cpu(self: ~T) -> ~T
 |      Moves all model parameters and buffers to the CPU.
 |
 |      .. note::
 |          This method modifies the module in-place.
 |
 |      Returns:
 |          Module: self
 |
 |  cuda(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T
 |      Moves all model parameters and buffers to the GPU.
 |
 |      This also makes associated parameters and buffers different objects. So
 |      it should be called before constructing optimizer if the module will
 |      live on GPU while being optimized.
 |
 |      .. note::
 |          This method modifies the module in-place.
 |
 |      Args:
 |          device (int, optional): if specified, all parameters will be
 |              copied to that device
 |
 |      Returns:
 |          Module: self
 |
 |  eval(self: ~T) -> ~T
 |      Sets the module in evaluation mode.
 |
 |      This has any effect only on certain modules. See documentations of
 |      particular modules for details of their behaviors in training/evaluation
 |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
 |      etc.
 |
 |      This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.
 |
 |      See :ref:`locally-disable-grad-doc` for a comparison between
 |      `.eval()` and several similar mechanisms that may be confused with it.
 |
 |      Returns:
 |          Module: self
 |
 |  extra_repr(self) -> str
 |      Set the extra representation of the module
 |
 |      To print customized extra information, you should re-implement
 |      this method in your own modules. Both single-line and multi-line
 |      strings are acceptable.
 |
 |  get_buffer(self, target: str) -> 'Tensor'
 |      Returns the buffer given by ``target`` if it exists,
 |      otherwise throws an error.
 |
 |      See the docstring for ``get_submodule`` for a more detailed
 |      explanation of this method's functionality as well as how to
 |      correctly specify ``target``.
 |
 |      Args:
 |          target: The fully-qualified string name of the buffer
 |              to look for. (See ``get_submodule`` for how to specify a
 |              fully-qualified string.)
 |
 |      Returns:
 |          torch.Tensor: The buffer referenced by ``target``
 |
 |      Raises:
 |          AttributeError: If the target string references an invalid
 |              path or resolves to something that is not a
 |              buffer
 |
 |  get_extra_state(self) -> Any
 |      Returns any extra state to include in the module's state_dict.
 |      Implement this and a corresponding :func:`set_extra_state` for your module
 |      if you need to store extra state. This function is called when building the
 |      module's `state_dict()`.
 |
 |      Note that extra state should be pickleable to ensure working serialization
 |      of the state_dict. We only provide provide backwards compatibility guarantees
 |      for serializing Tensors; other objects may break backwards compatibility if
 |      their serialized pickled form changes.
 |
 |      Returns:
 |          object: Any extra state to store in the module's state_dict
 |
 |  get_parameter(self, target: str) -> 'Parameter'
 |      Returns the parameter given by ``target`` if it exists,
 |      otherwise throws an error.
 |
 |      See the docstring for ``get_submodule`` for a more detailed
 |      explanation of this method's functionality as well as how to
 |      correctly specify ``target``.
 |
 |      Args:
 |          target: The fully-qualified string name of the Parameter
 |              to look for. (See ``get_submodule`` for how to specify a
 |              fully-qualified string.)
 |
 |      Returns:
 |          torch.nn.Parameter: The Parameter referenced by ``target``
 |
 |      Raises:
 |          AttributeError: If the target string references an invalid
 |              path or resolves to something that is not an
 |              ``nn.Parameter``
 |
 |  get_submodule(self, target: str) -> 'Module'
 |      Returns the submodule given by ``target`` if it exists,
 |      otherwise throws an error.
 |
 |      For example, let's say you have an ``nn.Module`` ``A`` that
 |      looks like this:
 |
 |      .. code-block:: text
 |
 |          A(
 |              (net_b): Module(
 |                  (net_c): Module(
 |                      (conv): Conv2d(16, 33, kernel_size=(3, 3), stride=(2, 2))
 |                  )
 |                  (linear): Linear(in_features=100, out_features=200, bias=True)
 |              )
 |          )
 |
 |      (The diagram shows an ``nn.Module`` ``A``. ``A`` has a nested
 |      submodule ``net_b``, which itself has two submodules ``net_c``
 |      and ``linear``. ``net_c`` then has a submodule ``conv``.)
 |
 |      To check whether or not we have the ``linear`` submodule, we
 |      would call ``get_submodule("net_b.linear")``. To check whether
 |      we have the ``conv`` submodule, we would call
 |      ``get_submodule("net_b.net_c.conv")``.
 |
 |      The runtime of ``get_submodule`` is bounded by the degree
 |      of module nesting in ``target``. A query against
 |      ``named_modules`` achieves the same result, but it is O(N) in
 |      the number of transitive modules. So, for a simple check to see
 |      if some submodule exists, ``get_submodule`` should always be
 |      used.
 |
 |      Args:
 |          target: The fully-qualified string name of the submodule
 |              to look for. (See above example for how to specify a
 |              fully-qualified string.)
 |
 |      Returns:
 |          torch.nn.Module: The submodule referenced by ``target``
 |
 |      Raises:
 |          AttributeError: If the target string references an invalid
 |              path or resolves to something that is not an
 |              ``nn.Module``
 |
 |  ipu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T
 |      Moves all model parameters and buffers to the IPU.
 |
 |      This also makes associated parameters and buffers different objects. So
 |      it should be called before constructing optimizer if the module will
 |      live on IPU while being optimized.
 |
 |      .. note::
 |          This method modifies the module in-place.
 |
 |      Arguments:
 |          device (int, optional): if specified, all parameters will be
 |              copied to that device
 |
 |      Returns:
 |          Module: self
 |
 |  load_state_dict(self, state_dict: Mapping[str, Any], strict: bool = True)
 |      Copies parameters and buffers from :attr:`state_dict` into
 |      this module and its descendants. If :attr:`strict` is ``True``, then
 |      the keys of :attr:`state_dict` must exactly match the keys returned
 |      by this module's :meth:`~torch.nn.Module.state_dict` function.
 |
 |      Args:
 |          state_dict (dict): a dict containing parameters and
 |              persistent buffers.
 |          strict (bool, optional): whether to strictly enforce that the keys
 |              in :attr:`state_dict` match the keys returned by this module's
 |              :meth:`~torch.nn.Module.state_dict` function. Default: ``True``
 |
 |      Returns:
 |          ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
 |              * **missing_keys** is a list of str containing the missing keys
 |              * **unexpected_keys** is a list of str containing the unexpected keys
 |
 |      Note:
 |          If a parameter or buffer is registered as ``None`` and its corresponding key
 |          exists in :attr:`state_dict`, :meth:`load_state_dict` will raise a
 |          ``RuntimeError``.
 |
 |  modules(self) -> Iterator[ForwardRef('Module')]
 |      Returns an iterator over all modules in the network.
 |
 |      Yields:
 |          Module: a module in the network
 |
 |      Note:
 |          Duplicate modules are returned only once. In the following
 |          example, ``l`` will be returned only once.
 |
 |      Example::
 |
 |          >>> l = nn.Linear(2, 2)
 |          >>> net = nn.Sequential(l, l)
 |          >>> for idx, m in enumerate(net.modules()):
 |                  print(idx, '->', m)
 |
 |          0 -> Sequential(
 |            (0): Linear(in_features=2, out_features=2, bias=True)
 |            (1): Linear(in_features=2, out_features=2, bias=True)
 |          )
 |          1 -> Linear(in_features=2, out_features=2, bias=True)
 |
 |  named_buffers(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.Tensor]]
 |      Returns an iterator over module buffers, yielding both the
 |      name of the buffer as well as the buffer itself.
 |
 |      Args:
 |          prefix (str): prefix to prepend to all buffer names.
 |          recurse (bool): if True, then yields buffers of this module
 |              and all submodules. Otherwise, yields only buffers that
 |              are direct members of this module.
 |
 |      Yields:
 |          (string, torch.Tensor): Tuple containing the name and buffer
 |
 |      Example::
 |
 |          >>> for name, buf in self.named_buffers():
 |          >>>    if name in ['running_var']:
 |          >>>        print(buf.size())
 |
 |  named_children(self) -> Iterator[Tuple[str, ForwardRef('Module')]]
 |      Returns an iterator over immediate children modules, yielding both
 |      the name of the module as well as the module itself.
 |
 |      Yields:
 |          (string, Module): Tuple containing a name and child module
 |
 |      Example::
 |
 |          >>> for name, module in model.named_children():
 |          >>>     if name in ['conv4', 'conv5']:
 |          >>>         print(module)
 |
 |  named_modules(self, memo: Optional[Set[ForwardRef('Module')]] = None, prefix: str = '', remove_duplicate: bool = True)
 |      Returns an iterator over all modules in the network, yielding
 |      both the name of the module as well as the module itself.
 |
 |      Args:
 |          memo: a memo to store the set of modules already added to the result
 |          prefix: a prefix that will be added to the name of the module
 |          remove_duplicate: whether to remove the duplicated module instances in the result
 |              or not
 |
 |      Yields:
 |          (string, Module): Tuple of name and module
 |
 |      Note:
 |          Duplicate modules are returned only once. In the following
 |          example, ``l`` will be returned only once.
 |
 |      Example::
 |
 |          >>> l = nn.Linear(2, 2)
 |          >>> net = nn.Sequential(l, l)
 |          >>> for idx, m in enumerate(net.named_modules()):
 |                  print(idx, '->', m)
 |
 |          0 -> ('', Sequential(
 |            (0): Linear(in_features=2, out_features=2, bias=True)
 |            (1): Linear(in_features=2, out_features=2, bias=True)
 |          ))
 |          1 -> ('0', Linear(in_features=2, out_features=2, bias=True))
 |
 |  named_parameters(self, prefix: str = '', recurse: bool = True) -> Iterator[Tuple[str, torch.nn.parameter.Parameter]]
 |      Returns an iterator over module parameters, yielding both the
 |      name of the parameter as well as the parameter itself.
 |
 |      Args:
 |          prefix (str): prefix to prepend to all parameter names.
 |          recurse (bool): if True, then yields parameters of this module
 |              and all submodules. Otherwise, yields only parameters that
 |              are direct members of this module.
 |
 |      Yields:
 |          (string, Parameter): Tuple containing the name and parameter
 |
 |      Example::
 |
 |          >>> for name, param in self.named_parameters():
 |          >>>    if name in ['bias']:
 |          >>>        print(param.size())
 |
 |  parameters(self, recurse: bool = True) -> Iterator[torch.nn.parameter.Parameter]
 |      Returns an iterator over module parameters.
 |
 |      This is typically passed to an optimizer.
 |
 |      Args:
 |          recurse (bool): if True, then yields parameters of this module
 |              and all submodules. Otherwise, yields only parameters that
 |              are direct members of this module.
 |
 |      Yields:
 |          Parameter: module parameter
 |
 |      Example::
 |
 |          >>> for param in model.parameters():
 |          >>>     print(type(param), param.size())
 |          <class 'torch.Tensor'> (20L,)
 |          <class 'torch.Tensor'> (20L, 1L, 5L, 5L)
 |
 |  register_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle
 |      Registers a backward hook on the module.
 |
 |      This function is deprecated in favor of :meth:`~torch.nn.Module.register_full_backward_hook` and
 |      the behavior of this function will change in future versions.
 |
 |      Returns:
 |          :class:`torch.utils.hooks.RemovableHandle`:
 |              a handle that can be used to remove the added hook by calling
 |              ``handle.remove()``
 |
 |  register_buffer(self, name: str, tensor: Optional[torch.Tensor], persistent: bool = True) -> None
 |      Adds a buffer to the module.
 |
 |      This is typically used to register a buffer that should not to be
 |      considered a model parameter. For example, BatchNorm's ``running_mean``
 |      is not a parameter, but is part of the module's state. Buffers, by
 |      default, are persistent and will be saved alongside parameters. This
 |      behavior can be changed by setting :attr:`persistent` to ``False``. The
 |      only difference between a persistent buffer and a non-persistent buffer
 |      is that the latter will not be a part of this module's
 |      :attr:`state_dict`.
 |
 |      Buffers can be accessed as attributes using given names.
 |
 |      Args:
 |          name (string): name of the buffer. The buffer can be accessed
 |              from this module using the given name
 |          tensor (Tensor or None): buffer to be registered. If ``None``, then operations
 |              that run on buffers, such as :attr:`cuda`, are ignored. If ``None``,
 |              the buffer is **not** included in the module's :attr:`state_dict`.
 |          persistent (bool): whether the buffer is part of this module's
 |              :attr:`state_dict`.
 |
 |      Example::
 |
 |          >>> self.register_buffer('running_mean', torch.zeros(num_features))
 |
 |  register_forward_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle
 |      Registers a forward hook on the module.
 |
 |      The hook will be called every time after :func:`forward` has computed an output.
 |      It should have the following signature::
 |
 |          hook(module, input, output) -> None or modified output
 |
 |      The input contains only the positional arguments given to the module.
 |      Keyword arguments won't be passed to the hooks and only to the ``forward``.
 |      The hook can modify the output. It can modify the input inplace but
 |      it will not have effect on forward since this is called after
 |      :func:`forward` is called.
 |
 |      Returns:
 |          :class:`torch.utils.hooks.RemovableHandle`:
 |              a handle that can be used to remove the added hook by calling
 |              ``handle.remove()``
 |
 |  register_forward_pre_hook(self, hook: Callable[..., NoneType]) -> torch.utils.hooks.RemovableHandle
 |      Registers a forward pre-hook on the module.
 |
 |      The hook will be called every time before :func:`forward` is invoked.
 |      It should have the following signature::
 |
 |          hook(module, input) -> None or modified input
 |
 |      The input contains only the positional arguments given to the module.
 |      Keyword arguments won't be passed to the hooks and only to the ``forward``.
 |      The hook can modify the input. User can either return a tuple or a
 |      single modified value in the hook. We will wrap the value into a tuple
 |      if a single value is returned(unless that value is already a tuple).
 |
 |      Returns:
 |          :class:`torch.utils.hooks.RemovableHandle`:
 |              a handle that can be used to remove the added hook by calling
 |              ``handle.remove()``
 |
 |  register_full_backward_hook(self, hook: Callable[[ForwardRef('Module'), Union[Tuple[torch.Tensor, ...], torch.Tensor], Union[Tuple[torch.Tensor, ...], torch.Tensor]], Optional[torch.Tensor]]) -> torch.utils.hooks.RemovableHandle
 |      Registers a backward hook on the module.
 |
 |      The hook will be called every time the gradients with respect to module
 |      inputs are computed. The hook should have the following signature::
 |
 |          hook(module, grad_input, grad_output) -> tuple(Tensor) or None
 |
 |      The :attr:`grad_input` and :attr:`grad_output` are tuples that contain the gradients
 |      with respect to the inputs and outputs respectively. The hook should
 |      not modify its arguments, but it can optionally return a new gradient with
 |      respect to the input that will be used in place of :attr:`grad_input` in
 |      subsequent computations. :attr:`grad_input` will only correspond to the inputs given
 |      as positional arguments and all kwarg arguments are ignored. Entries
 |      in :attr:`grad_input` and :attr:`grad_output` will be ``None`` for all non-Tensor
 |      arguments.
 |
 |      For technical reasons, when this hook is applied to a Module, its forward function will
 |      receive a view of each Tensor passed to the Module. Similarly the caller will receive a view
 |      of each Tensor returned by the Module's forward function.
 |
 |      .. warning ::
 |          Modifying inputs or outputs inplace is not allowed when using backward hooks and
 |          will raise an error.
 |
 |      Returns:
 |          :class:`torch.utils.hooks.RemovableHandle`:
 |              a handle that can be used to remove the added hook by calling
 |              ``handle.remove()``
 |
 |  register_load_state_dict_post_hook(self, hook)
 |      Registers a post hook to be run after module's ``load_state_dict``
 |      is called.
 |
 |      It should have the following signature::
 |          hook(module, incompatible_keys) -> None
 |
 |      The ``module`` argument is the current module that this hook is registered
 |      on, and the ``incompatible_keys`` argument is a ``NamedTuple`` consisting
 |      of attributes ``missing_keys`` and ``unexpected_keys``. ``missing_keys``
 |      is a ``list`` of ``str`` containing the missing keys and
 |      ``unexpected_keys`` is a ``list`` of ``str`` containing the unexpected keys.
 |
 |      The given incompatible_keys can be modified inplace if needed.
 |
 |      Note that the checks performed when calling :func:`load_state_dict` with
 |      ``strict=True`` are affected by modifications the hook makes to
 |      ``missing_keys`` or ``unexpected_keys``, as expected. Additions to either
 |      set of keys will result in an error being thrown when ``strict=True``, and
 |      clearning out both missing and unexpected keys will avoid an error.
 |
 |      Returns:
 |          :class:`torch.utils.hooks.RemovableHandle`:
 |              a handle that can be used to remove the added hook by calling
 |              ``handle.remove()``
 |
 |  register_module(self, name: str, module: Optional[ForwardRef('Module')]) -> None
 |      Alias for :func:`add_module`.
 |
 |  register_parameter(self, name: str, param: Optional[torch.nn.parameter.Parameter]) -> None
 |      Adds a parameter to the module.
 |
 |      The parameter can be accessed as an attribute using given name.
 |
 |      Args:
 |          name (string): name of the parameter. The parameter can be accessed
 |              from this module using the given name
 |          param (Parameter or None): parameter to be added to the module. If
 |              ``None``, then operations that run on parameters, such as :attr:`cuda`,
 |              are ignored. If ``None``, the parameter is **not** included in the
 |              module's :attr:`state_dict`.
 |
 |  requires_grad_(self: ~T, requires_grad: bool = True) -> ~T
 |      Change if autograd should record operations on parameters in this
 |      module.
 |
 |      This method sets the parameters' :attr:`requires_grad` attributes
 |      in-place.
 |
 |      This method is helpful for freezing part of the module for finetuning
 |      or training parts of a model individually (e.g., GAN training).
 |
 |      See :ref:`locally-disable-grad-doc` for a comparison between
 |      `.requires_grad_()` and several similar mechanisms that may be confused with it.
 |
 |      Args:
 |          requires_grad (bool): whether autograd should record operations on
 |                                parameters in this module. Default: ``True``.
 |
 |      Returns:
 |          Module: self
 |
 |  set_extra_state(self, state: Any)
 |      This function is called from :func:`load_state_dict` to handle any extra state
 |      found within the `state_dict`. Implement this function and a corresponding
 |      :func:`get_extra_state` for your module if you need to store extra state within its
 |      `state_dict`.
 |
 |      Args:
 |          state (dict): Extra state from the `state_dict`
 |
 |  share_memory(self: ~T) -> ~T
 |      See :meth:`torch.Tensor.share_memory_`
 |
 |  to(self, *args, **kwargs)
 |      Moves and/or casts the parameters and buffers.
 |
 |      This can be called as
 |
 |      .. function:: to(device=None, dtype=None, non_blocking=False)
 |         :noindex:
 |
 |      .. function:: to(dtype, non_blocking=False)
 |         :noindex:
 |
 |      .. function:: to(tensor, non_blocking=False)
 |         :noindex:
 |
 |      .. function:: to(memory_format=torch.channels_last)
 |         :noindex:
 |
 |      Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
 |      floating point or complex :attr:`dtype`\ s. In addition, this method will
 |      only cast the floating point or complex parameters and buffers to :attr:`dtype`
 |      (if given). The integral parameters and buffers will be moved
 |      :attr:`device`, if that is given, but with dtypes unchanged. When
 |      :attr:`non_blocking` is set, it tries to convert/move asynchronously
 |      with respect to the host if possible, e.g., moving CPU Tensors with
 |      pinned memory to CUDA devices.
 |
 |      See below for examples.
 |
 |      .. note::
 |          This method modifies the module in-place.
 |
 |      Args:
 |          device (:class:`torch.device`): the desired device of the parameters
 |              and buffers in this module
 |          dtype (:class:`torch.dtype`): the desired floating point or complex dtype of
 |              the parameters and buffers in this module
 |          tensor (torch.Tensor): Tensor whose dtype and device are the desired
 |              dtype and device for all parameters and buffers in this module
 |          memory_format (:class:`torch.memory_format`): the desired memory
 |              format for 4D parameters and buffers in this module (keyword
 |              only argument)
 |
 |      Returns:
 |          Module: self
 |
 |      Examples::
 |
 |          >>> linear = nn.Linear(2, 2)
 |          >>> linear.weight
 |          Parameter containing:
 |          tensor([[ 0.1913, -0.3420],
 |                  [-0.5113, -0.2325]])
 |          >>> linear.to(torch.double)
 |          Linear(in_features=2, out_features=2, bias=True)
 |          >>> linear.weight
 |          Parameter containing:
 |          tensor([[ 0.1913, -0.3420],
 |                  [-0.5113, -0.2325]], dtype=torch.float64)
 |          >>> gpu1 = torch.device("cuda:1")
 |          >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
 |          Linear(in_features=2, out_features=2, bias=True)
 |          >>> linear.weight
 |          Parameter containing:
 |          tensor([[ 0.1914, -0.3420],
 |                  [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
 |          >>> cpu = torch.device("cpu")
 |          >>> linear.to(cpu)
 |          Linear(in_features=2, out_features=2, bias=True)
 |          >>> linear.weight
 |          Parameter containing:
 |          tensor([[ 0.1914, -0.3420],
 |                  [-0.5112, -0.2324]], dtype=torch.float16)
 |
 |          >>> linear = nn.Linear(2, 2, bias=None).to(torch.cdouble)
 |          >>> linear.weight
 |          Parameter containing:
 |          tensor([[ 0.3741+0.j,  0.2382+0.j],
 |                  [ 0.5593+0.j, -0.4443+0.j]], dtype=torch.complex128)
 |          >>> linear(torch.ones(3, 2, dtype=torch.cdouble))
 |          tensor([[0.6122+0.j, 0.1150+0.j],
 |                  [0.6122+0.j, 0.1150+0.j],
 |                  [0.6122+0.j, 0.1150+0.j]], dtype=torch.complex128)
 |
 |  to_empty(self: ~T, *, device: Union[str, torch.device]) -> ~T
 |      Moves the parameters and buffers to the specified device without copying storage.
 |
 |      Args:
 |          device (:class:`torch.device`): The desired device of the parameters
 |              and buffers in this module.
 |
 |      Returns:
 |          Module: self
 |
 |  train(self: ~T, mode: bool = True) -> ~T
 |      Sets the module in training mode.
 |
 |      This has any effect only on certain modules. See documentations of
 |      particular modules for details of their behaviors in training/evaluation
 |      mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
 |      etc.
 |
 |      Args:
 |          mode (bool): whether to set training mode (``True``) or evaluation
 |                       mode (``False``). Default: ``True``.
 |
 |      Returns:
 |          Module: self
 |
 |  xpu(self: ~T, device: Union[int, torch.device, NoneType] = None) -> ~T
 |      Moves all model parameters and buffers to the XPU.
 |
 |      This also makes associated parameters and buffers different objects. So
 |      it should be called before constructing optimizer if the module will
 |      live on XPU while being optimized.
 |
 |      .. note::
 |          This method modifies the module in-place.
 |
 |      Arguments:
 |          device (int, optional): if specified, all parameters will be
 |              copied to that device
 |
 |      Returns:
 |          Module: self
 |
 |  zero_grad(self, set_to_none: bool = False) -> None
 |      Sets gradients of all model parameters to zero. See similar function
 |      under :class:`torch.optim.Optimizer` for more context.
 |
 |      Args:
 |          set_to_none (bool): instead of setting to zero, set the grads to None.
 |              See :meth:`torch.optim.Optimizer.zero_grad` for details.
 |
 |  ----------------------------------------------------------------------
 |  Data descriptors inherited from torch.nn.modules.module.Module:
 |
 |  __dict__
 |      dictionary for instance variables (if defined)
 |
 |  __weakref__
 |      list of weak references to the object (if defined)
 |
 |  ----------------------------------------------------------------------
 |  Data and other attributes inherited from torch.nn.modules.module.Module:
 |
 |  T_destination = ~T_destination
 |
 |  dump_patches = False
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pytorch_lightning-1.7.0.dev0-py3.9.egg/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pytorch_lightning-1.7.0.dev0-py3.9.egg/pytorch_lightning/trainer/connectors/accelerator_connector.py:424: LightningDeprecationWarning: Setting `Trainer(gpus=-1)` is deprecated in v1.7 and will be removed in v2.0. Please use `Trainer(accelerator='gpu', devices=-1)` instead.
  rank_zero_deprecation(
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
GPU available: False, used: False
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./MNIST/raw/train-images-idx3-ubyte.gz
Extracting ./MNIST/raw/train-images-idx3-ubyte.gz to ./MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./MNIST/raw/train-labels-idx1-ubyte.gz
Extracting ./MNIST/raw/train-labels-idx1-ubyte.gz to ./MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./MNIST/raw/t10k-images-idx3-ubyte.gz
Extracting ./MNIST/raw/t10k-images-idx3-ubyte.gz to ./MNIST/raw
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./MNIST/raw/t10k-labels-idx1-ubyte.gz
Extracting ./MNIST/raw/t10k-labels-idx1-ubyte.gz to ./MNIST/raw
  | Name     | Type     | Params
--------------------------------------
0 | layer_1  | Linear   | 100 K
1 | layer_2  | Linear   | 33.0 K
2 | layer_3  | Linear   | 2.6 K
3 | accuracy | Accuracy | 0
--------------------------------------
136 K     Trainable params
0         Non-trainable params
136 K     Total params
0.544     Total estimated model params size (MB)
/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pytorch_lightning-1.7.0.dev0-py3.9.egg/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pytorch_lightning-1.7.0.dev0-py3.9.egg/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/opt/homebrew/Caskroom/miniforge/base/lib/python3.9/site-packages/pytorch_lightning-1.7.0.dev0-py3.9.egg/pytorch_lightning/trainer/connectors/data_connector.py:219: PossibleUserWarning: The dataloader, test_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 10 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(